"""
æ— å½±èˆ†æƒ…çˆ¬å–æœåŠ¡ä¸»å…¥å£
æœ¬æŠ€èƒ½ä»…è´Ÿè´£æŒ‰å…³é”®è¯/å¹³å°çˆ¬å–åŸå§‹æ•°æ®å¹¶è¿”å›ï¼›æƒ…æ„Ÿåˆ†æã€æŠ¥å‘Šç”Ÿæˆç”±ä¸» Agent å®Œæˆã€‚
é™¤ AGENTBAY_API_KEY å¤–ï¼Œå…¶ä½™å‚æ•°ç”±ä¸» Agent é€šè¿‡ä¼ å‚/å‘½ä»¤è¡Œä¼ å…¥ã€‚
"""
import sys
import json
from pathlib import Path
from datetime import datetime

# ä»æŠ€èƒ½æ ¹ç›®å½•è¿è¡Œ python scripts/crawl.py æ—¶ï¼Œå°† scripts åŠ å…¥ path ä»¥ä¾¿å¯¼å…¥
_scripts_dir = Path(__file__).resolve().parent
if str(_scripts_dir) not in sys.path:
    sys.path.insert(0, str(_scripts_dir))

import asyncio
import argparse
import os
from typing import List, Optional, Dict, Any

from crawler import SocialMediaCrawler, get_platform_config, create_crawler_session
from reporter import ReportGenerator


def get_api_key():
    """ä» ~/.config/agentbay/api_key æˆ–ç¯å¢ƒå˜é‡ AGENTBAY_API_KEY è·å– API Key"""
    from pathlib import Path
    file_path = Path.home() / ".config" / "agentbay" / "api_key"
    file_path.parent.mkdir(parents=True, exist_ok=True)
    if not file_path.exists():
        file_path.touch()
    if os.environ.get("AGENTBAY_API_KEY"):
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(os.environ.get("AGENTBAY_API_KEY"))
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            api_key = f.read().strip()
        if not api_key:
            api_key = None
    except Exception as e:
        api_key = None
    return api_key


# ---------- ä»¥ä¸‹ä¾›ä¸» Agent åœ¨çˆ¬å–å®Œæˆåè°ƒç”¨ï¼šæŠ¥å‘Šç”Ÿæˆ ----------


def generate_report(
    processed_results: Dict[str, Any],
    output_dir: str = "output",
    title: Optional[str] = None,
) -> Dict[str, Any]:
    """
    æ ¹æ®æƒ…æ„Ÿåˆ†æç»“æœç”ŸæˆæŠ¥å‘Šï¼ˆMarkdown/JSON/å¯é€‰ PDFï¼‰ï¼Œä¾›ä¸» Agent è°ƒç”¨ã€‚

    Args:
        processed_results: ä¸» Agent æŒ‰æç¤ºè¯å®Œæˆæƒ…æ„Ÿåˆ†æåå†™å…¥çš„ JSONï¼ˆéœ€ç¬¦åˆ sentiment_instruction.md ä¸­çš„è¾“å‡ºæ ¼å¼ï¼‰
        output_dir: æŠ¥å‘Šè¾“å‡ºç›®å½•
        title: æŠ¥å‘Šæ ‡é¢˜ï¼Œå¯é€‰

    Returns:
        å« markdown_pathã€json_pathã€pdf_pathï¼ˆè‹¥æœ‰ï¼‰ç­‰çš„å­—å…¸
    """
    generator = ReportGenerator(output_dir=output_dir)
    return generator.generate_report(processed_results=processed_results, title=title)


def _compute_crawl_timeout(keywords: List[str], max_results_per_keyword: int) -> int:
    """æŒ‰ã€Œçº¦ 1 æ¡/åˆ†é’Ÿã€åŠ¨æ€è®¡ç®—çˆ¬å–è¶…æ—¶ï¼ˆç§’ï¼‰ï¼Œè‡³å°‘ 10 åˆ†é’Ÿã€‚"""
    total_max = len(keywords) * max_results_per_keyword
    return max(1200, 180 + total_max * 60)


async def crawl_for_sentiment(
    platform: str,
    keywords: List[str],
    *,
    max_results_per_keyword: int = 10,
    output_dir: Optional[str] = None,
    report_title: Optional[str] = None,
    agentbay_api_key: Optional[str] = None,
    context_name: str = "sentiment-analysis",
    crawl_timeout: Optional[int] = None,
) -> Dict[str, Any]:
    """
    æŒ‰å…³é”®è¯/å¹³å°çˆ¬å–åŸå§‹æ•°æ®å¹¶è¿”å›ã€‚ä¸åœ¨æ­¤åšæƒ…æ„Ÿåˆ†ææˆ–æŠ¥å‘Šç”Ÿæˆï¼Œç”±ä¸» Agent åŸºäºè¿”å›æ•°æ®å®Œæˆã€‚

    Args:
        platform: å¹³å°æ ‡è¯†ï¼ˆ"xhs", "weibo", "bing" ç­‰ï¼‰
        keywords: å…³é”®è¯åˆ—è¡¨
        max_results_per_keyword: æ¯ä¸ªå…³é”®è¯çš„æœ€å¤§ç»“æœæ•°
        output_dir: åŸå§‹æ•°æ®è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ "output"
        report_title: ç”¨äºç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼Œå¯é€‰
        agentbay_api_key: AgentBay API Keyï¼Œæœªä¼ åˆ™ä»ç¯å¢ƒå˜é‡ AGENTBAY_API_KEY è¯»å–
        context_name: Browser Context åç§°
        crawl_timeout: çˆ¬å–è¶…æ—¶ï¼ˆç§’ï¼‰ï¼›ä¸ä¼ æˆ–ä¸º None æ—¶æŒ‰ã€Œçº¦ 1 æ¡/åˆ†é’Ÿã€åŠ¨æ€è®¡ç®—ï¼Œè‡³å°‘ 10 åˆ†é’Ÿ

    Returns:
        å« successã€crawl_resultsã€raw_output_pathï¼ˆå¯é€‰ï¼‰çš„å­—å…¸
    """
    if crawl_timeout is None or crawl_timeout <= 0:
        crawl_timeout = _compute_crawl_timeout(keywords, max_results_per_keyword)
        print(f"â±ï¸ çˆ¬å–è¶…æ—¶å·²åŠ¨æ€è®¾ç½®ä¸º {crawl_timeout} ç§’ï¼ˆçº¦ 1 æ¡/åˆ†é’Ÿï¼Œè‡³å°‘ 10 åˆ†é’Ÿï¼‰\n")

    api_key = (agentbay_api_key or "").strip() or get_api_key()
    if not api_key:
        return {
            "success": False,
            "error": "æœªæä¾› AGENTBAY_API_KEYï¼ˆè¯·è®¾ç½®ç¯å¢ƒå˜é‡ AGENTBAY_API_KEY æˆ–åˆ›å»º ~/.config/agentbay/api_key æ–‡ä»¶ï¼‰",
        }

    if not keywords:
        return {"success": False, "error": "å…³é”®è¯ keywords ä¸èƒ½ä¸ºç©º"}

    out_dir = output_dir or "output"

    print(f"\n{'='*60}")
    print(f"ğŸš€ å¯åŠ¨èˆ†æƒ…çˆ¬å–ï¼ˆæƒ…æ„Ÿåˆ†æç”±ä¸» Agent å®Œæˆï¼‰")
    print(f"{'='*60}\n")

    try:
        platform_config = get_platform_config(platform)
        print(f"ğŸ“± ç›®æ ‡å¹³å°: {platform_config.display_name}")
    except ValueError as e:
        return {"success": False, "error": str(e)}

    print(f"ğŸ” å…³é”®è¯: {', '.join(keywords)}")
    print(f"ğŸ“Š æ¯ä¸ªå…³é”®è¯æœ€å¤§ç»“æœæ•°: {max_results_per_keyword}\n")

    adapter = None
    try:
        print("=" * 60)
        print("æ­¥éª¤1: åˆ›å»ºçˆ¬å–ä¼šè¯")
        print("=" * 60)
        adapter = await create_crawler_session(
            api_key=api_key,
            context_name=context_name,
            platform_config=platform_config,
        )

        print("\n" + "=" * 60)
        print("æ­¥éª¤2: æ‰§è¡Œå†…å®¹çˆ¬å–")
        print("=" * 60)
        crawler = SocialMediaCrawler(adapter, platform_config)

        if len(keywords) == 1:
            crawl_results = await crawler.crawl_by_keyword(
                keyword=keywords[0],
                max_results=max_results_per_keyword,
                timeout=crawl_timeout,
            )
        else:
            crawl_results = await crawler.crawl_multiple_keywords(
                keywords=keywords,
                max_results_per_keyword=max_results_per_keyword,
                timeout=crawl_timeout,
            )

        if not crawl_results.get("success"):
            return crawl_results

        crawl_results["data_sources"] = [crawl_results.get("platform_display", "æµè§ˆå™¨çˆ¬å–")]

        # å°†åŸå§‹çˆ¬å–ç»“æœå†™å…¥ output_dirï¼Œä¾›ä¸» Agent åšæƒ…æ„Ÿåˆ†æ/æŠ¥å‘Š
        raw_output_path = None
        try:
            out_path = Path(out_dir)
            out_path.mkdir(parents=True, exist_ok=True)
            platform_display = crawl_results.get("platform_display", platform)
            title_part = (report_title or f"{platform_display}_{','.join(keywords[:2])}").replace(" ", "_")[:50]
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            raw_filename = f"{platform}_{title_part}_{ts}.json"
            raw_output_path = out_path / raw_filename
            with open(raw_output_path, "w", encoding="utf-8") as f:
                json.dump(crawl_results, f, ensure_ascii=False, indent=2)
            raw_output_path = str(raw_output_path)
        except Exception as e:
            print(f"   âš ï¸ å†™å…¥åŸå§‹æ•°æ®æ–‡ä»¶å¤±è´¥ï¼ˆä¸å½±å“è¿”å›ï¼‰: {e}")

        print("\n" + "=" * 60)
        print("âœ… çˆ¬å–å®Œæˆï¼ŒåŸå§‹æ•°æ®å·²è¿”å›ï¼ˆæƒ…æ„Ÿåˆ†æç”±ä¸» Agent å®Œæˆï¼‰")
        print("=" * 60)

        return {
            "success": True,
            "crawl_results": crawl_results,
            "raw_output_path": raw_output_path,
        }

    except Exception as e:
        import traceback
        error_msg = f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        print(f"\nâŒ {error_msg}")
        print(f"è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
        return {"success": False, "error": error_msg}

    finally:
        if adapter:
            await adapter.close()


def _parse_args():
    parser = argparse.ArgumentParser(
        description="èˆ†æƒ…çˆ¬å–ï¼šé™¤ AGENTBAY_API_KEY å¤–ï¼Œå…¶ä½™å‚æ•°ç”±ä¸» Agent ä¼ å…¥ï¼›æƒ…æ„Ÿåˆ†æç”±ä¸» Agent å®Œæˆã€‚",
    )
    parser.add_argument("--keywords", "-k", required=True, help="å…³é”®è¯ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”")
    parser.add_argument("--platform", "-p", default="baidu", help="å¹³å°: baidu/xhs/weibo/douyin/zhihu/bingï¼ˆé»˜è®¤ baiduï¼‰")
    parser.add_argument("--max-results", type=int, default=10, help="æ¯å…³é”®è¯æœ€å¤§ç»“æœæ•°ï¼ˆé»˜è®¤10ï¼‰")
    parser.add_argument("--output-dir", "-o", default="output", help="æŠ¥å‘Šè¾“å‡ºç›®å½•")
    parser.add_argument("--report-title", help="æŠ¥å‘Šæ ‡é¢˜ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--context-name", default="sentiment-analysis", help="Browser Context åç§°")
    parser.add_argument(
        "--crawl-timeout",
        type=int,
        default=None,
        help="çˆ¬å–è¶…æ—¶ï¼ˆç§’ï¼‰ï¼›ä¸ä¼ æ—¶æŒ‰ã€Œçº¦ 1 æ¡/åˆ†é’Ÿã€åŠ¨æ€è®¡ç®—ï¼Œè‡³å°‘ 10 åˆ†é’Ÿ",
    )
    return parser.parse_args()


async def main():
    args = _parse_args()
    keywords = [kw.strip() for kw in args.keywords.split(",") if kw.strip()]
    if not keywords:
        print("âŒ é”™è¯¯: --keywords ä¸èƒ½ä¸ºç©º")
        sys.exit(1)

    # æœªä¼  --crawl-timeout æ—¶ç”± crawl_for_sentiment å†…éƒ¨æŒ‰ã€Œçº¦ 1 æ¡/åˆ†é’Ÿã€åŠ¨æ€è®¡ç®—
    effective_timeout = args.crawl_timeout
    if effective_timeout is None:
        effective_timeout = _compute_crawl_timeout(keywords, args.max_results)
    print(f"\nğŸ“‹ å‚æ•°: å¹³å°={args.platform}, å…³é”®è¯={', '.join(keywords)}, æ¯å…³é”®è¯æœ€å¤§={args.max_results}, è¾“å‡º={args.output_dir}, çˆ¬å–è¶…æ—¶={effective_timeout} ç§’")
    if args.report_title:
        print(f"   æŠ¥å‘Šæ ‡é¢˜: {args.report_title}")
    print()

    result = await crawl_for_sentiment(
        platform=args.platform,
        keywords=keywords,
        max_results_per_keyword=args.max_results,
        output_dir=args.output_dir,
        report_title=args.report_title or None,
        context_name=args.context_name,
        crawl_timeout=args.crawl_timeout,
    )

    if result.get("success"):
        print("\nâœ… çˆ¬å–å®Œæˆï¼")
        path = result.get("raw_output_path")
        if path:
            print(f"ğŸ“„ åŸå§‹æ•°æ®: {path}")
        print("   æƒ…æ„Ÿåˆ†æã€æŠ¥å‘Šç”±ä¸» Agent åŸºäºè¿”å›æ•°æ®å®Œæˆã€‚")
    else:
        print(f"\nâŒ çˆ¬å–å¤±è´¥: {result.get('error')}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
